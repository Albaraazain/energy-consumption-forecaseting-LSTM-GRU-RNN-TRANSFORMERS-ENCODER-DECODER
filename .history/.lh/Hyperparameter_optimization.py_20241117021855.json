{
    "sourceFile": "Hyperparameter_optimization.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1731799045091,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1731799081097,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,11 +1,10 @@\n+from RNN import Simple_RNN\r\n import torch\r\n from Preprocessing import CustomDataset, process_missing_and_duplicate_timestamps\r\n from torch.utils.data import DataLoader\r\n import optuna\r\n \r\n-from RNNBasedModels.RNN import Simple_RNN\r\n-\r\n device = (\r\n     \"mps\"\r\n     if getattr(torch, \"has_mps\", False)\r\n     else \"cuda\"\r\n@@ -15,9 +14,9 @@\n \r\n input_sequence_length = 168\r\n target_sequence_length = 48\r\n \r\n-path = \"Datasets//DUQ_hourly.csv\"\r\n+path = \"Datasets/DUQ_hourly.csv\"\r\n train_set , val_set , test_set = process_missing_and_duplicate_timestamps(filepath = path)\r\n \r\n train_dataset = CustomDataset(train_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\n train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False , drop_last=True)\r\n"
                }
            ],
            "date": 1731799045091,
            "name": "Commit-0",
            "content": "import torch\r\nfrom Preprocessing import CustomDataset, process_missing_and_duplicate_timestamps\r\nfrom torch.utils.data import DataLoader\r\nimport optuna\r\n\r\nfrom RNNBasedModels.RNN import Simple_RNN\r\n\r\ndevice = (\r\n    \"mps\"\r\n    if getattr(torch, \"has_mps\", False)\r\n    else \"cuda\"\r\n    if torch.cuda.is_available()\r\n    else \"cpu\"\r\n)\r\n\r\ninput_sequence_length = 168\r\ntarget_sequence_length = 48\r\n\r\npath = \"Datasets//DUQ_hourly.csv\"\r\ntrain_set , val_set , test_set = process_missing_and_duplicate_timestamps(filepath = path)\r\n\r\ntrain_dataset = CustomDataset(train_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=False , drop_last=True)\r\ntest_dataset = CustomDataset(test_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False , drop_last=True)\r\nval_dataset = CustomDataset(val_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False , drop_last=True)\r\n\r\ndef objective(trial):\r\n    # Define the search space for hyperparameters\r\n    num_layers = trial.suggest_int('num_layers', 1, 10)\r\n    hid_dim = trial.suggest_int('hid_dim', 16, 2048)\r\n    drop_rate = trial.suggest_float('drop_rate', 0.0, 0.5)\r\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\r\n\r\n    # Define your model and optimizer based on the suggested hyperparameters\r\n    model = Simple_RNN(in_dim = 1, hid_dim = hid_dim , out_dim = 1, num_layers = num_layers, drop_rate = drop_rate).to(device)\r\n\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n    criterion = torch.nn.MSELoss()  # Adjust this based on your specific loss function\r\n\r\n    # Training loop within the objective function\r\n    for epoch in range(10):  # Assuming epochs is defined outside the function\r\n        model.train()\r\n        for batch in train_loader:\r\n            x_batch, y_batch = batch\r\n\r\n            optimizer.zero_grad()\r\n            out= model(x_batch)\r\n            train_loss = criterion(out, y_batch)\r\n            train_loss.backward()\r\n            optimizer.step()\r\n\r\n        # Validation loop to track performance on the validation set (not provided in your original code)\r\n        model.eval()\r\n        test_losses = []\r\n        with torch.no_grad():\r\n            for batch in test_loader:\r\n                x_batch, y_batch = batch\r\n                out= model(x_batch)\r\n                test_loss = criterion(out, y_batch)\r\n                test_losses.append(test_loss.item())\r\n\r\n        # Calculate and report the average validation loss for this epoch\r\n        avg_test_loss = sum(test_losses) / len(test_losses)\r\n        trial.report(avg_test_loss, epoch)\r\n\r\n        # Pruning based on the intermediate result; stop early if the performance is not improving\r\n        if trial.should_prune():\r\n            raise optuna.TrialPruned()\r\n\r\n    return avg_test_loss  # Return the final validation loss after all epochs\r\n\r\n\r\nstudy = optuna.create_study(direction='minimize')\r\nstudy.optimize(objective, n_trials=100)\r\n\r\n\r\nbest_params = study.best_params\r\nbest_num_layers = best_params['num_layers']\r\nbest_hid_dim = best_params['hid_dim']\r\nbest_drop_rate = best_params['drop_rate']\r\nbest_learning_rate = best_params['learning_rate']\r\n\r\nprint(\"Best hyperparameters found:\")\r\nprint(f\"Num layers: {best_num_layers}\")\r\nprint(f\"Hid dim: {best_hid_dim}\")\r\nprint(f\"Drop rate: {best_drop_rate}\")\r\nprint(f\"Learning rate: {best_learning_rate}\")\r\n\r\n\r\n#Best hyperparameters found:\r\n#Num layers: 1\r\n#Hid dim: 1740\r\n#Drop rate: 0.0009001480178615212\r\n#Learning rate: 0.0012874179807017348\r\n\r\n\r\n'''\r\n\r\nIn this study, we employed the Optuna to identify optimal parameters suitable for our dataset\r\nand model complexity. Specifically, we configured Optuna to conduct 100 trials with 20 internal\r\nepochs. Parameters were set within defined ranges: the number of layers varied from 1 to 10, \r\nhidden layers spanned from 16 to 2048, drop probabilities ranged between 0 and 1, and learning \r\nrates fell within the range of 0 to 1, consistent across all five models. However, certain models \r\nnecessitated additional hyperparameters; for instance, in the case of the 1D-CNN, we specified the \r\n𝑘𝑒𝑟𝑛𝑒𝑙 𝑠𝑖𝑧𝑒 between 3 − 7 and 𝑝𝑎𝑑𝑑𝑖𝑛𝑔 as (𝑘𝑒𝑟𝑛𝑒𝑙_𝑠𝑖𝑧𝑒 − 1) // 2. Similarly, for the transformers, t\r\nhe required number of ℎ𝑒𝑎𝑑𝑠 set from 1 to 8, and the number of dimensions from 32 to 1024. \r\nAdditionally, we manually set the activation function as linear, \r\nthe number of 𝑒𝑝𝑜𝑐ℎ𝑠, 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 and 𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑟 in Table 3 of article. \r\n\r\n'''\r\n\r\n"
        }
    ]
}