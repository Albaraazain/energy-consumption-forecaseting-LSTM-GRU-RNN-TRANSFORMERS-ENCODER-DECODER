{
    "sourceFile": ".history/Preprocessing_20241117021800.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1731799135997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1731799135997,
            "name": "Commit-0",
            "content": "import pandas as pd\r\nimport numpy as np\r\n\r\nimport torch\r\n\r\nfrom torch.utils.data import TensorDataset , DataLoader, Dataset\r\n\r\ntry:\r\n    import google.colab\r\n    COLAB = True\r\n    print(\"Note: using Google CoLab\")\r\nexcept:\r\n    print(\"Note: not using Google CoLab\")\r\n    COLAB = False\r\n\r\ndevice = (\r\n    \"mps\"\r\n    if getattr(torch, \"has_mps\", False)\r\n    else \"cuda\"\r\n    if torch.cuda.is_available()\r\n    else \"cpu\"\r\n)\r\nprint(f\"Using device: {device}\")\r\n\r\n\r\n\r\ndef is_ne_in_df(df:pd.DataFrame):\r\n    for col in df.columns:\r\n        true_bool = (df[col] == \"n/e\")\r\n        if any(true_bool):\r\n            return True\r\n    return False\r\n\r\ndef to_numeric_and_downcast_data(df: pd.DataFrame):\r\n    fcols = df.select_dtypes('float').columns\r\n    icols = df.select_dtypes('integer').columns\r\n    df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')\r\n    df[icols] = df[icols].apply(pd.to_numeric, downcast='integer')\r\n    return df\r\n\r\ndef process_missing_and_duplicate_timestamps(filepath, train_size=80, val_size=50, verbose=True):\r\n    df = pd.read_csv(filepath)\r\n    df.sort_values('Datetime', inplace=True)\r\n    df.reset_index(drop=True, inplace=True)\r\n\r\n    indices_to_remove = []\r\n    rows_to_add = []\r\n    hour_counter = 1\r\n    prev_date = ''\r\n\r\n    if verbose:\r\n        print(filepath)\r\n\r\n    for index, row in df.iterrows():\r\n        date_str = row['Datetime']\r\n\r\n        year_str = date_str[0:4]\r\n        month_str = date_str[5:7]\r\n        day_str = date_str[8:10]\r\n        hour_str = date_str[11:13]\r\n        tail_str = date_str[14:]\r\n\r\n        def date_to_str():\r\n            return '-'.join([year_str, month_str, day_str]) + ' ' + ':'.join([hour_str, tail_str])\r\n\r\n        def date_with_hour(hour):\r\n            hour = '0' + str(hour) if hour < 10 else str(hour)\r\n            return '-'.join([year_str, month_str, day_str]) + ' ' + ':'.join([hour, tail_str])\r\n\r\n        if hour_counter != int(hour_str):\r\n            if prev_date == date_to_str():\r\n                # Duplicate datetime, calculate the average and keep only one\r\n                average = int((df.iat[index, 1] + df.iat[index - 1, 1]) / 2)  # Get the average\r\n                df.iat[index, 1] = average\r\n                indices_to_remove.append(index - 1)\r\n                if verbose:\r\n                    print('Duplicate ' + date_to_str() + ' with average ' + str(average))\r\n            elif hour_counter < 23:\r\n                # Missing datetime, add it using the average of the previous and next for the consumption (MWs)\r\n                average = int((df.iat[index, 1] + df.iat[index - 1, 1]) / 2)\r\n                rows_to_add.append(pd.Series([date_with_hour(hour_counter), average], index=df.columns))\r\n                if verbose:\r\n                    print('Missing ' + date_with_hour(hour_counter) + ' with average ' + str(average))\r\n            else:\r\n                print(date_to_str() + ' and hour_counter ' + str(hour_counter) + \" with previous: \" + prev_date)\r\n\r\n            # Adjust for the missing/duplicate value\r\n            if prev_date < date_to_str():\r\n                hour_counter = (hour_counter + 1) % 24\r\n            else:\r\n                hour_counter = (hour_counter - 1) if hour_counter - 1 > 0 else 0\r\n\r\n        # Increment the hour\r\n        hour_counter = (hour_counter + 1) % 24\r\n        prev_date = date_str\r\n\r\n    df.drop(indices_to_remove, inplace=True)\r\n    if rows_to_add:\r\n        new_rows = pd.concat(rows_to_add, axis=1).transpose()\r\n        df = pd.concat([df, new_rows], ignore_index=True)  # Concatenating the new rows\r\n\r\n    # New rows are added at the end, sort them and also recalculate the indices\r\n    df.sort_values('Datetime', inplace=True)\r\n    df.reset_index(drop=True, inplace=True)\r\n    df = df.set_index(\"Datetime\")\r\n    if is_ne_in_df(df):\r\n        raise ValueError(\"data frame contains 'n/e' values. These must be handled\")\r\n    df = to_numeric_and_downcast_data(df)\r\n    data_mean = df.mean(axis=0)\r\n    data_std = df.std(axis=0)\r\n    df = (df - data_mean) / data_std\r\n    stats = (data_mean, data_std)\r\n\r\n    train = df[:len(df) * train_size // 100]\r\n    val = df[len(train) : len(train) + ((len(df) - len(train)) * val_size) // 100]\r\n    test = df[len(val) + len(train) : ]\r\n\r\n    #scaler = MinMaxScaler()\r\n    #train = scaler.fit_transform(train)\r\n    #val = scaler.fit_transform(val)\r\n    #test = scaler.fit_transform(test)\r\n\r\n    return np.array(train) , np.array(val) , np.array(test)\r\n\r\n\r\npath = \"Datasets/DUQ_hourly.csv\"\r\ntrain_set , val_set , test_set = process_missing_and_duplicate_timestamps(filepath = path)\r\n\r\n\r\nclass CustomDataset(Dataset):\r\n    def __init__(self, sequence, input_sequence_length, target_sequence_length, multivariate=False, target_feature=None):\r\n        self.sequence = sequence\r\n        self.input_sequence_length = input_sequence_length\r\n        self.target_sequence_length = target_sequence_length\r\n        self.window_size = input_sequence_length + target_sequence_length\r\n        self.multivariate = multivariate\r\n        self.target_feature = target_feature\r\n\r\n    def __len__(self):\r\n        return len(self.sequence) - self.window_size + 1\r\n\r\n    def __getitem__(self, idx):\r\n        src = self.sequence[idx:idx + self.input_sequence_length]\r\n        trg = self.sequence[idx + self.input_sequence_length - 1:idx + self.window_size -1]\r\n\r\n        if self.multivariate:\r\n            trg_y = [obs[self.target_feature] for obs in self.sequence[idx + self.input_sequence_length:idx + self.input_sequence_length + self.target_sequence_length]]\r\n            trg_y = torch.tensor(trg_y).unsqueeze(1).to(device)  # Adding a dimension for sequence length\r\n        else:\r\n            trg_y = self.sequence[idx + self.input_sequence_length:idx + self.input_sequence_length + self.target_sequence_length]\r\n            trg_y = torch.tensor(trg_y).to(device)  # Adding a dimension for sequence length\r\n\r\n        src = torch.tensor(src).to(device)  # Adding a dimension for features\r\n        trg = torch.tensor(trg).to(device)  # Adding a dimension for features\r\n\r\n        return src, trg, trg_y\r\n    \r\n\r\ninput_sequence_length = 168\r\ntarget_sequence_length = 48\r\n\r\ntrain_dataset = CustomDataset(train_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=False , drop_last=True)\r\ntest_dataset = CustomDataset(test_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False , drop_last=True)\r\nval_dataset = CustomDataset(val_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False , drop_last=True)\r\n\r\n"
        }
    ]
}