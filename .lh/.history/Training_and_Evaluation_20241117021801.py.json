{
    "sourceFile": ".history/Training_and_Evaluation_20241117021801.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1731799136000,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1731799136000,
            "name": "Commit-0",
            "content": "import torch\r\nfrom matplotlib import pyplot as plt\r\nfrom torch import nn, Tensor\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\nimport numpy as np\r\n\r\nfrom AttentionBasedModels.Decoder_only import Decoder\r\nfrom AttentionBasedModels.Enc_and_Dec_Transformer import Transformer\r\nfrom AttentionBasedModels.Encoder_only import Encoder\r\nfrom Preprocessing import CustomDataset, process_missing_and_duplicate_timestamps\r\nfrom torch.utils.data import TensorDataset , DataLoader, Dataset\r\nimport pandas as pd\r\nfrom tqdm import tqdm\r\n\r\nfrom RNNBasedModels.GRU import GRU\r\nfrom RNNBasedModels.LSTM import LSTM\r\nfrom RNNBasedModels.RNN import Simple_RNN\r\n\r\ndevice = (\r\n    \"mps\"\r\n    if getattr(torch, \"has_mps\", False)\r\n    else \"cuda\"\r\n    if torch.cuda.is_available()\r\n    else \"cpu\"\r\n)\r\n\r\n\r\ninput_sequence_length = 168\r\ntarget_sequence_length = 48\r\n\r\npath = \"Datasets/DUQ_hourly.csv\"\r\ntrain_set , val_set , test_set = process_missing_and_duplicate_timestamps(filepath = path)\r\n\r\ntrain_dataset = CustomDataset(train_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=False , drop_last=True)\r\ntest_dataset = CustomDataset(test_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False , drop_last=True)\r\nval_dataset = CustomDataset(val_set, input_sequence_length, target_sequence_length, multivariate=False, target_feature=0)\r\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False , drop_last=True)\r\n\r\n# 1. RNN training step\r\n\r\n\r\n\r\nmodel = Simple_RNN(in_dim = 1, hid_dim = 1740, out_dim = 1, num_layers = 1, drop_rate = 0.0009001480178615212).to(device)\r\nmodel\r\n\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0012874179807017348)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\r\n\r\nepochs = 200\r\nearly_stop_count = 0\r\nmin_val_loss = float('inf')\r\ntrain_loss = []\r\nvalidation_loss = []\r\nfor epoch in range(epochs):\r\n    model.train()\r\n    for batch in train_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch, y_batch = x_batch, y_batch\r\n\r\n        optimizer.zero_grad()\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        loss.backward()\r\n        optimizer.step()\r\n    model.eval()\r\n    val_losses = []\r\n    with torch.no_grad():\r\n        for batch in test_loader:\r\n            x_batch, y_batch = batch\r\n            x_batch, y_batch = x_batch, y_batch\r\n            outputs = model(x_batch)\r\n            loss = criterion(outputs, y_batch)\r\n            val_losses.append(loss.item())\r\n\r\n    val_loss = np.mean(val_losses)\r\n    scheduler.step(val_loss)\r\n\r\n    if val_loss < min_val_loss:\r\n        min_val_loss = val_loss\r\n        early_stop_count = 0\r\n    else:\r\n        early_stop_count += 1\r\n\r\n    if early_stop_count >= 20:\r\n        print(\"Early stopping!\")\r\n        break\r\n    train_loss.append(loss)\r\n    validation_loss.append(val_loss)\r\n    print(f\"Epoch {epoch + 1}/{epochs}, train loss: {loss}, validation Loss: {val_loss}\")\r\n\r\ntrain_losses_np = torch.tensor(train_loss).detach().cpu().numpy()\r\ntest_loss_np = torch.tensor(validation_loss).detach().cpu().numpy()\r\n\r\nmodel.eval()\r\npredictions = []\r\nactual = []\r\ntest_loss = []\r\nwith torch.no_grad():\r\n      for batch in val_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch = x_batch\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        test_loss.append(loss.detach().cpu().numpy())\r\n        actual.append(y_batch.detach().cpu().numpy())\r\n        predictions.append(outputs.detach().cpu().numpy())\r\n\r\npredictions = np.vstack(predictions)\r\nactual = np.vstack(actual)\r\ntest_loss = np.vstack(test_loss)\r\n\r\ndef mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n\r\n   return mean_diff\r\n\r\nprint(f\"MSE : {mean_squared_error(actual , predictions)}\")\r\n\r\ndef root_mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n   rmse_val = np.sqrt(mean_diff)\r\n   return rmse_val\r\nprint(f\"RMSE : {root_mean_squared_error(actual , predictions)}\")\r\n\r\ndef mean_absolute_error(act, pred):\r\n    diff = pred - act\r\n    abs_diff = np.absolute(diff)\r\n    mean_diff = abs_diff.mean()\r\n    return mean_diff\r\nprint(f\"MAE : {mean_absolute_error(actual , predictions)}\")\r\n\r\nk = pd.DataFrame(predictions.squeeze(-1))\r\nm = pd.DataFrame(actual.squeeze(-1))\r\n\r\nplt.plot([x for x in range(96)], k[0][:96], label = \"Predicted\")\r\nplt.plot([x for x in range(96)] , m[0][:96], label = \"Actual\")\r\nplt.title(\"RNN Predicted vs. Actual Value\")\r\nplt.legend(loc = \"upper right\")\r\n\r\n\r\nplt.plot([x for x in range(train_losses_np.size)] , train_losses_np, label = \"train loss\")\r\nplt.plot([x for x in range(test_loss_np.size)] , test_loss_np , label = \"val loss\")\r\n#plt.plot([x for x in range(len(val_losses[:200]))] , val_losses[:200] , label = \"val loss\")\r\nplt.title(\"RNN train vs. val loss\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"losses\")\r\nplt.legend()\r\n\r\n\r\n# 2. LSTM training step\r\n\r\n\r\nmodel = LSTM(input_size = 1, hidden_size =100 , num_stacked_layers = 3, drop_rate = 0.1).to(device)\r\n\r\n\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\r\n\r\n\r\nepochs = 200\r\nearly_stop_count = 0\r\nmin_val_loss = float('inf')\r\ntrain_loss = []\r\nvalidation_loss = []\r\nfor epoch in tqdm(range(epochs)):\r\n    model.train()\r\n    for batch in train_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch, y_batch = x_batch, y_batch\r\n\r\n        optimizer.zero_grad()\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        loss.backward()\r\n        optimizer.step()\r\n    model.eval()\r\n    val_losses = []\r\n    with torch.no_grad():\r\n        for batch in test_loader:\r\n            x_batch, y_batch = batch\r\n            x_batch, y_batch = x_batch, y_batch\r\n            outputs = model(x_batch)\r\n            loss = criterion(outputs, y_batch)\r\n            val_losses.append(loss.item())\r\n\r\n    val_loss = np.mean(val_losses)\r\n    scheduler.step(val_loss)\r\n\r\n    if val_loss < min_val_loss:\r\n        min_val_loss = val_loss\r\n        early_stop_count = 0\r\n    else:\r\n        early_stop_count += 1\r\n\r\n    if early_stop_count >= 20:\r\n        print(\"Early stopping!\")\r\n        break\r\n    train_loss.append(loss)\r\n    validation_loss.append(val_loss)\r\n    print(f\"Epoch {epoch + 1}/{epochs}, Train loss: {loss},Validation Loss: {val_loss}\")\r\n    \r\ntrain_losses_np = torch.tensor(train_loss).detach().cpu().numpy()\r\ntest_loss_np = torch.tensor(validation_loss).detach().cpu().numpy()\r\n\r\nmodel.eval()\r\npredictions = []\r\nactual = []\r\ntest_loss = []\r\nwith torch.no_grad():\r\n    for batch in val_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch = x_batch\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        test_loss.append(loss.detach().cpu().numpy())\r\n        actual.append(y_batch.detach().cpu().numpy())\r\n        predictions.append(outputs.detach().cpu().numpy())\r\n        \r\npredictions = np.vstack(predictions)\r\nactual = np.vstack(actual)\r\ntest_loss = np.vstack(test_loss)\r\n\r\ndef mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n\r\n   return mean_diff\r\n\r\nprint(f\"MSE : {mean_squared_error(actual , predictions)}\")\r\n\r\n\r\ndef root_mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n   rmse_val = np.sqrt(mean_diff)\r\n   return rmse_val\r\n\r\n\r\nprint(f\"RMSE : {root_mean_squared_error(actual , predictions)}\")\r\n\r\ndef mean_absolute_error(act, pred):\r\n    diff = pred - act\r\n    abs_diff = np.absolute(diff)\r\n    mean_diff = abs_diff.mean()\r\n    return mean_diff\r\n\r\nprint(f\"MAE : {mean_absolute_error(actual , predictions)}\")\r\n\r\nk = pd.DataFrame(predictions.squeeze(-1))\r\nm = pd.DataFrame(actual.squeeze(-1))\r\n\r\nplt.plot([x for x in range(96)], k[0][:96], label = \"Predicted\")\r\nplt.plot([x for x in range(96)] , m[0][:96], label = \"Actual\")\r\nplt.title(\"First 96 LSTM Predicted vs Actual value\")\r\nplt.legend()\r\n\r\nplt.plot([x for x in range(train_losses_np.size)] , train_losses_np, label = \"train loss\")\r\nplt.plot([x for x in range(test_loss_np.size)] , test_loss_np , label = \"val loss\")\r\n#plt.plot([x for x in range(len(val_losses[0:epoch+1]))] , val_losses[0:epoch+1] , label = \"val loss\")\r\nplt.title(\"LSTM train vs. val loss\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"Losses\")\r\nplt.legend()\r\n\r\n\r\n\r\n# 3. GRU training step\r\n\r\nmodel = GRU(in_dim = 7, hid_dim = 100, out_dim = 1, num_layer = 3 , drop_rate = 0.1).to(device)\r\n\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\r\n\r\nepochs = 200\r\nearly_stop_count = 0\r\nmin_test_loss = float('inf')\r\ntrain_loss = []\r\ntest_loss1 = []\r\nfor epoch in range(epochs):\r\n    model.train()\r\n    for batch in train_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch, y_batch = x_batch, y_batch\r\n\r\n        optimizer.zero_grad()\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        loss.backward()\r\n        optimizer.step()\r\n    model.eval()\r\n    test_losses = []\r\n    with torch.no_grad():\r\n        for batch in val_loader:\r\n            x_batch, y_batch = batch\r\n            x_batch, y_batch = x_batch, y_batch\r\n            outputs = model(x_batch)\r\n            loss = criterion(outputs, y_batch)\r\n            test_losses.append(loss.item())\r\n\r\n    test_loss = np.mean(test_losses)\r\n    scheduler.step(test_loss)\r\n\r\n    if test_loss < min_test_loss:\r\n        min_test_loss = test_loss\r\n        early_stop_count = 0\r\n    else:\r\n        early_stop_count += 1\r\n\r\n    if early_stop_count >= 20:\r\n        print(\"Early stopping!\")\r\n        break\r\n    train_loss.append(loss)\r\n    test_loss1.append(test_loss)\r\n    print(f\"Epoch {epoch + 1}/{epochs}, train loss: {loss}, val loss: {test_loss}\")\r\n    \r\ntrain_losses_np = torch.tensor(train_loss).detach().cpu().numpy()\r\ntest_loss_np = torch.tensor(test_loss1).detach().cpu().numpy()\r\n\r\nmodel.eval()\r\npredictions = []\r\nactual = []\r\ntest_loss = []\r\nwith torch.no_grad():\r\n    for batch in test_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch = x_batch\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        test_loss.append(loss.detach().cpu().numpy())\r\n        actual.append(y_batch.detach().cpu().numpy())\r\n        predictions.append(outputs.detach().cpu().numpy())\r\n        \r\npredictions = np.vstack(predictions)\r\nactual = np.vstack(actual)\r\ntest_loss = np.vstack(test_loss)\r\n\r\ndef mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n\r\n   return mean_diff\r\n\r\nprint(f\"MSE : {mean_squared_error(actual , predictions)}\")\r\n\r\ndef root_mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n   rmse_val = np.sqrt(mean_diff)\r\n   return rmse_val\r\n\r\nprint(f\"RMSE : {root_mean_squared_error(actual , predictions)}\")\r\n\r\ndef mean_absolute_error(act, pred):\r\n    diff = pred - act\r\n    abs_diff = np.absolute(diff)\r\n    mean_diff = abs_diff.mean()\r\n    return mean_diff\r\n\r\nprint(f\"MAE : {mean_absolute_error(actual , predictions)}\")\r\n\r\nk = pd.DataFrame(predictions.squeeze(-1))\r\nm = pd.DataFrame(actual.squeeze(-1))\r\n\r\nplt.plot([x for x in range(169)], k[0][:169], label = \"Predicted\")\r\nplt.plot([x for x in range(10)] , m[0][:10], label = \"Actual\")\r\nplt.title(\"Multivariate GRU Predicted vs Actual value for next 2 dayes\")\r\nplt.legend()\r\n\r\nplt.plot([x for x in range(train_losses_np.size)] , train_losses_np, label = \"train loss\")\r\nplt.plot([x for x in range(test_loss_np.size)] , test_loss_np , label = \"val loss\")\r\n#plt.plot([x for x in range(len(val_losses[0:epoch+1]))] , val_losses[0:epoch+1] , label = \"val loss\")\r\nplt.title(\"Multivariate GRU train vs. val losses\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"Losses\")\r\nplt.legend()\r\n\r\nfrom CNNBasedModels.OneD_CNN import CNN_ForecastNet\r\n# 4. 1D-CNN training step\r\n\r\nmodel = CNN_ForecastNet(hidden_size=100, kernel_size=5, padding=2, drop_rate=0.1).to(device)\r\n\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\r\n\r\nepochs = 200\r\nearly_stop_count = 0\r\nmin_val_loss = float('inf')\r\ntrain_loss = []\r\nvalidation_loss = []\r\nfor epoch in tqdm(range(epochs)):\r\n    model.train()\r\n    for batch in train_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch, y_batch = x_batch, y_batch\r\n\r\n        optimizer.zero_grad()\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        loss.backward()\r\n        optimizer.step()\r\n    model.eval()\r\n    val_losses = []\r\n    with torch.no_grad():\r\n        for batch in test_loader:\r\n            x_batch, y_batch = batch\r\n            x_batch, y_batch = x_batch, y_batch\r\n            outputs = model(x_batch)\r\n            loss = criterion(outputs, y_batch)\r\n            val_losses.append(loss.item())\r\n\r\n    val_loss = np.mean(val_losses)\r\n    scheduler.step(val_loss)\r\n\r\n    if val_loss < min_val_loss:\r\n        min_val_loss = val_loss\r\n        early_stop_count = 0\r\n    else:\r\n        early_stop_count += 1\r\n\r\n    if early_stop_count >= 20:\r\n        print(\"Early stopping!\")\r\n        break\r\n    train_loss.append(loss)\r\n    validation_loss.append(val_loss)\r\n    print(f\"Epoch {epoch + 1}/{epochs}, Train loss: {loss},Validation Loss: {val_loss}\")\r\n\r\ntrain_losses_np = torch.tensor(train_loss).detach().cpu().numpy()\r\ntest_loss_np = torch.tensor(validation_loss).detach().cpu().numpy()\r\n\r\n\r\nmodel.eval()\r\npredictions = []\r\nactual = []\r\ntest_loss = []\r\nwith torch.no_grad():\r\n    for batch in val_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch = x_batch\r\n        outputs = model(x_batch)\r\n        loss = criterion(outputs, y_batch)\r\n        test_loss.append(loss.detach().cpu().numpy())\r\n        actual.append(y_batch.detach().cpu().numpy())\r\n        predictions.append(outputs.detach().cpu().numpy())\r\n        \r\npredictions = np.vstack(predictions)\r\nactual = np.vstack(actual)\r\ntest_loss = np.vstack(test_loss)\r\n\r\ndef mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n\r\n   return mean_diff\r\n\r\nprint(f\"MSE : {mean_squared_error(actual , predictions)}\")\r\n\r\ndef root_mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n   rmse_val = np.sqrt(mean_diff)\r\n   return rmse_val\r\n\r\n\r\nprint(f\"RMSE : {root_mean_squared_error(actual , predictions)}\")\r\n\r\ndef mean_absolute_error(act, pred):\r\n    diff = pred - act\r\n    abs_diff = np.absolute(diff)\r\n    mean_diff = abs_diff.mean()\r\n    return mean_diff\r\n\r\nprint(f\"MAE : {mean_absolute_error(actual , predictions)}\")\r\n\r\nk = pd.DataFrame(predictions.squeeze(-1))\r\nm = pd.DataFrame(actual.squeeze(-1))\r\n\r\nplt.plot([x for x in range(96)], k[0][:96], label = \"Predicted\")\r\nplt.plot([x for x in range(96)] , m[0][:96], label = \"Actual\")\r\nplt.title(\"First 96 LSTM Predicted vs Actual value\")\r\nplt.legend()\r\n\r\nplt.plot([x for x in range(train_losses_np.size)] , train_losses_np, label = \"train loss\")\r\nplt.plot([x for x in range(test_loss_np.size)] , test_loss_np , label = \"val loss\")\r\n#plt.plot([x for x in range(len(val_losses[0:epoch+1]))] , val_losses[0:epoch+1] , label = \"val loss\")\r\nplt.title(\"LSTM train vs. val loss\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"Losses\")\r\nplt.legend()\r\n\r\n\r\n# 5. Encoder-only training step\r\n\r\n\r\nmodel = Encoder(num_layers=1, D=32, H=1, hidden_mlp_dim=100,\r\n                                       inp_features=1, out_features=1, dropout_rate=0.1).to(device)\r\n\r\n\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\r\n\r\nepochs = 50\r\nearly_stop_count = 0\r\nmin_test_loss = float('inf')\r\n\r\n\r\ntrain_losses = []\r\ntest_losses1 = []\r\n\r\nfor epoch in tqdm(range(epochs)):\r\n    model.train()\r\n    for batch in train_loader:\r\n        x_batch, y_batch = batch\r\n        x_batch = x_batch.to(device)\r\n        y_batch = y_batch.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        out, _ = model(x_batch)\r\n        train_loss = criterion(y_batch , out)\r\n        train_loss.backward()\r\n        optimizer.step()\r\n\r\n    model.eval()\r\n    test_losses = []\r\n    with torch.no_grad():\r\n        for batch in test_loader:\r\n            x_batch, y_batch = batch\r\n            x_batch = x_batch.to(device)\r\n            y_batch = y_batch.to(device)\r\n\r\n            out, _ = model(x_batch)\r\n            test_loss = criterion(y_batch , out)\r\n            test_losses.append(test_loss.item())\r\n\r\n    test_loss = np.mean(test_losses)\r\n    scheduler.step(test_loss)\r\n\r\n    if test_loss < min_test_loss:\r\n        min_test_loss = test_loss\r\n        early_stop_count = 0\r\n    else:\r\n        early_stop_count += 1\r\n\r\n    if early_stop_count >= 5:\r\n        print(\"Early stopping!\")\r\n        break\r\n    train_losses.append(train_loss)\r\n    test_losses1.append(test_loss)\r\n    print(f\"Epoch {epoch + 1}/{epochs}, train loss: {train_loss}, test Loss: {test_loss}\")\r\n\r\n\r\ntrain_losses_np = torch.tensor(train_losses).detach().cpu().numpy()\r\ntest_loss_np = torch.tensor(test_losses1).detach().cpu().numpy()\r\n\r\nval_losses, val_preds, y_val_batch  = [], [] , []\r\nmodel.eval()\r\nfor (x, y) in val_loader:\r\n    x = x.to(device)\r\n    y = y.to(device)\r\n    y_pred, _ = model(x)\r\n    loss_test = criterion(y_pred, y)  # (B,S)\r\n    val_losses.append(loss_test.item())\r\n    y_val_batch.append(y.detach().cpu().numpy())\r\n    val_preds.append(y_pred.detach().cpu().numpy())\r\nval_predicted = np.vstack(val_preds)\r\nactual_val_batch = np.vstack(y_val_batch)\r\n\r\ndef mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n\r\n   return mean_diff\r\n\r\nprint(f\"MSE : {mean_squared_error(actual_val_batch , val_predicted)}\")\r\n\r\ndef root_mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n   rmse_val = np.sqrt(mean_diff)\r\n   return rmse_val\r\n\r\n\r\nprint(f\"RMSE : {root_mean_squared_error(actual_val_batch , val_predicted)}\")\r\n\r\ndef mean_absolute_error(act, pred):\r\n    diff = pred - act\r\n    abs_diff = np.absolute(diff)\r\n    mean_diff = abs_diff.mean()\r\n    return mean_diff\r\n\r\nprint(f\"MAE : {mean_absolute_error(actual_val_batch , val_predicted)}\")\r\n\r\nk = pd.DataFrame(val_predicted.squeeze(-1))\r\nm = pd.DataFrame(actual_val_batch.squeeze(-1))\r\n\r\nplt.plot([x for x in range(96)], k[0][:96], label = \"Predicted\")\r\nplt.plot([x for x in range(96)] , m[0][:96], label = \"Actual\")\r\n#plt.title(\"First 96 Only-Ecoder Predicted vs. Actual\")\r\nplt.legend()\r\n\r\ntp = train_losses_np*10\r\n\r\nplt.plot([x for x in range(train_losses_np.size)] , tp, label = \"train loss\")\r\nplt.plot([x for x in range(test_loss_np.size)] , test_loss_np , label = \"val loss\")\r\n#plt.plot([x for x in range(len(val_losses[0:epoch+1]))] , val_losses[0:epoch+1] , label = \"val loss\")\r\nplt.title(\"Encoder train vs. val loss\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"Losses\")\r\nplt.legend()\r\n\r\n\r\n# 6. Decoder-only training step\r\n\r\nmodel = Decoder(num_layers=1, D=32, H=4, hidden_mlp_dim=32,\r\n                                       inp_features=1, out_features=1, dropout_rate=0.1).to(device)\r\n\r\ndef create_look_ahead_mask(size, device=device):\r\n    mask = torch.ones((size, size), device=device)\r\n    mask = torch.triu(mask, diagonal=1)\r\n    return mask  # (size, size)\r\n\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\r\n\r\nepochs = 200\r\nearly_stop_count = 0\r\nmin_test_loss = float('inf')\r\n\r\n#x_batch , y_batch = next(iter(train_loader))\r\n#y_batch.unsqueeze(-1).shape\r\n\r\ntrain_losses = []\r\ntest_losses1 = []\r\n\r\nfor epoch in tqdm(range(epochs)):\r\n    model.train()\r\n    for batch in train_loader:\r\n        x_batch, y_batch = batch\r\n\r\n        S = x_batch.shape[1]\r\n        mask = create_look_ahead_mask(S)\r\n        optimizer.zero_grad()\r\n        out, _ = model(x_batch, mask)\r\n        train_loss = criterion(y_batch , out)\r\n        train_loss.backward()\r\n        optimizer.step()\r\n\r\n    model.eval()\r\n    test_losses = []\r\n    with torch.no_grad():\r\n        for batch in test_loader:\r\n            x_batch, y_batch = batch\r\n            S = x_batch.shape[1]\r\n            mask = create_look_ahead_mask(S)\r\n            out, _ = model(x_batch , mask)\r\n            test_loss = criterion(y_batch , out)\r\n            test_losses.append(test_loss.item())\r\n\r\n    test_loss = np.mean(test_losses)\r\n    scheduler.step(test_loss)\r\n\r\n    if test_loss < min_test_loss:\r\n        min_test_loss = test_loss\r\n        early_stop_count = 0\r\n    else:\r\n        early_stop_count += 1\r\n\r\n    if early_stop_count >= 20:\r\n        print(\"Early stopping!\")\r\n        break\r\n    train_losses.append(train_loss)\r\n    test_losses1.append(test_loss)\r\n    print(f\"Epoch {epoch + 1}/{epochs}, train loss: {train_loss}, test Loss: {test_loss}\")\r\n    \r\ntrain_losses_np = torch.tensor(train_losses).detach().cpu().numpy()\r\ntest_loss_np = torch.tensor(test_losses1).detach().cpu().numpy()\r\n\r\nval_losses, val_preds, y_val_batch  = [], [] , []\r\nmodel.eval()\r\nfor (x, y) in val_loader:\r\n    x = x.to(device)\r\n    y = y.to(device)\r\n    S = x.shape[-2]\r\n    y_pred, _ = model(x, mask=create_look_ahead_mask(S))\r\n    loss_test = criterion(y_pred, y)  # (B,S)\r\n    val_losses.append(loss_test.item())\r\n    y_val_batch.append(y.detach().cpu().numpy())\r\n    val_preds.append(y_pred.detach().cpu().numpy())\r\nval_predicted = np.vstack(val_preds)\r\nactual_val_batch = np.vstack(y_val_batch)\r\n\r\ndef mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n\r\n   return mean_diff\r\n\r\nprint(f\"MSE : {mean_squared_error(actual_val_batch , val_predicted)}\")\r\n\r\ndef root_mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n   rmse_val = np.sqrt(mean_diff)\r\n   return rmse_val\r\n\r\n\r\nprint(f\"RMSE : {root_mean_squared_error(actual_val_batch , val_predicted)}\")\r\n\r\ndef mean_absolute_error(act, pred):\r\n    diff = pred - act\r\n    abs_diff = np.absolute(diff)\r\n    mean_diff = abs_diff.mean()\r\n    return mean_diff\r\n\r\nprint(f\"MAE : {mean_absolute_error(actual_val_batch , val_predicted)}\")\r\n\r\nk = pd.DataFrame(val_predicted.squeeze(-1))\r\nm = pd.DataFrame(actual_val_batch.squeeze(-1))\r\n\r\nplt.plot([x for x in range(48)], k[0][:48], label = \"Predicted\")\r\nplt.plot([x for x in range(48)] , m[0][:48], label = \"Actual\")\r\nplt.title(\"First 48 Decoder Predicted vs. Actual\")\r\nplt.legend()\r\n\r\nplt.plot([x for x in range(train_losses_np.size)] , train_losses_np, label = \"train loss\")\r\nplt.plot([x for x in range(test_loss_np.size)] , test_loss_np , label = \"val loss\")\r\n#plt.plot([x for x in range(len(val_losses[0:epoch+1]))] , val_losses[0:epoch+1] , label = \"val loss\")\r\nplt.title(\"Decoder train vs. val loss\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"Losses\")\r\nplt.legend()\r\n\r\n\r\n# 7. Enc&Dec Transformer training step\r\n\r\n\r\n\r\n\r\ndef generate_square_subsequent_mask(dim1: int, dim2: int , device = device) -> Tensor:\r\n    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\r\n\r\n\r\nmodel = Transformer(\r\n    input_size=1,\r\n    dec_seq_len=48,\r\n    num_predicted_features=1\r\n    ).to(device)\r\n\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\r\n\r\nepochs = 200\r\nearly_stop_count = 0\r\nmin_test_loss = float('inf')\r\n\r\ntrain_losses = []\r\ntest_losses1 = []\r\n\r\nfor epoch in tqdm(range(epochs)):\r\n    model.train()\r\n    for batch in train_loader:\r\n        src, trg , trg_y = batch\r\n\r\n        tgt_mask = generate_square_subsequent_mask(\r\n            dim1=target_sequence_length,\r\n            dim2=target_sequence_length\r\n           ).to(device)\r\n        src_mask = generate_square_subsequent_mask(\r\n            dim1=target_sequence_length,\r\n            dim2=input_sequence_length\r\n           ).to(device)\r\n\r\n        optimizer.zero_grad()\r\n        out = model(src , trg, src_mask , tgt_mask)\r\n        train_loss = criterion(trg_y , out)\r\n        train_loss.backward()\r\n        optimizer.step()\r\n\r\n    model.eval()\r\n    test_losses = []\r\n    with torch.no_grad():\r\n        for batch in test_loader:\r\n            src, trg , trg_y = batch\r\n            tgt_mask = generate_square_subsequent_mask(\r\n                dim1=target_sequence_length,\r\n                dim2=target_sequence_length\r\n               ).to(device)\r\n            src_mask = generate_square_subsequent_mask(\r\n                dim1=target_sequence_length,\r\n                dim2=input_sequence_length\r\n               ).to(device)\r\n\r\n            out = model(src , trg , src_mask , tgt_mask)\r\n            test_loss = criterion(trg_y , out)\r\n            test_losses.append(test_loss.item())\r\n\r\n    test_loss = np.mean(test_losses)\r\n    scheduler.step(test_loss)\r\n\r\n    if test_loss < min_test_loss:\r\n        min_test_loss = test_loss\r\n        early_stop_count = 0\r\n    else:\r\n        early_stop_count += 1\r\n\r\n    if early_stop_count >= 20:\r\n        print(\"Early stopping!\")\r\n        break\r\n    train_losses.append(train_loss)\r\n    test_losses1.append(test_loss)\r\n    print(f\"Epoch {epoch + 1}/{epochs}, train loss: {train_loss}, val Loss: {test_loss}\")\r\n    \r\ntrain_losses_np = torch.tensor(train_losses).detach().cpu().numpy()\r\ntest_loss_np = torch.tensor(test_losses1).detach().cpu().numpy()\r\n\r\nval_losses, val_preds, y_val_batch  = [], [] , []\r\nmodel.eval()\r\nfor batch in val_loader:\r\n            src, trg , trg_y = batch\r\n            tgt_mask = generate_square_subsequent_mask(\r\n                dim1=target_sequence_length,\r\n                dim2=target_sequence_length\r\n               ).to(device)\r\n            src_mask = generate_square_subsequent_mask(\r\n                dim1=target_sequence_length,\r\n                dim2=input_sequence_length\r\n               ).to(device)\r\n\r\n            out = model(src , trg , src_mask , tgt_mask)\r\n            val_loss = criterion(trg_y , out)\r\n            val_losses.append(val_loss.item())\r\n            val_preds.append(out.detach().cpu().numpy())\r\n            y_val_batch.append(trg_y.detach().cpu().numpy())\r\nval_predicted = np.vstack(val_preds)\r\nactual_val_batch = np.vstack(y_val_batch)\r\n\r\ndef mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n\r\n   return mean_diff\r\n\r\nprint(f\"MSE : {mean_squared_error(actual_val_batch , val_predicted)}\")\r\n\r\ndef root_mean_squared_error(act, pred):\r\n\r\n   diff = pred - act\r\n   differences_squared = diff ** 2\r\n   mean_diff = differences_squared.mean()\r\n   rmse_val = np.sqrt(mean_diff)\r\n   return rmse_val\r\n\r\n\r\nprint(f\"RMSE : {root_mean_squared_error(actual_val_batch , val_predicted)}\")\r\n\r\ndef mean_absolute_error(act, pred):\r\n    diff = pred - act\r\n    abs_diff = np.absolute(diff)\r\n    mean_diff = abs_diff.mean()\r\n    return mean_diff\r\n\r\nprint(f\"MAE : {mean_absolute_error(actual_val_batch , val_predicted)}\")\r\n\r\nk = pd.DataFrame(val_predicted.squeeze(-1))\r\nm = pd.DataFrame(actual_val_batch.squeeze(-1))\r\n\r\nplt.plot([x for x in range(96)], k[0][:96], label = \"Predicted\")\r\nplt.plot([x for x in range(96)] , m[0][:96], label = \"Actual\")\r\nplt.title(\"First 96 Transformer Predicted vs. Actual\")\r\nplt.legend(loc = \"upper right\")\r\nplt.show()\r\n\r\nplt.plot([x for x in range(train_losses_np.size)] , train_losses_np, label = \"train loss\")\r\nplt.plot([x for x in range(test_loss_np.size)] , test_loss_np , label = \"val loss\")\r\n#plt.plot([x for x in range(len(val_losses[0:epoch+1]))] , val_losses[0:epoch+1] , label = \"val loss\")\r\nplt.title(\"Transformers train vs. val loss\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"Losses\")\r\nplt.legend()\r\n"
        }
    ]
}